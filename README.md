# Capita Selecta â€“ Text Analysis & Topic Modeling

This project explores **web scraping, natural language processing, and topic modeling** on Italian text.  
It uses **Python**, **spaCy**, **scikit-learn**, and **LDA** to extract, clean, and analyze text data.  

## ğŸ“Œ Features

- Web scraping with [`requests-html`](https://requests-html.kennethreitz.org/)  
- Text preprocessing (stopword removal, tokenization, lemmatization)  
- Topic modeling using **Latent Dirichlet Allocation (LDA)**  
- Interactive visualization with **pyLDAvis**  
- Data exploration with **pandas**, **matplotlib**, and **seaborn**

## ğŸ› ï¸ Installation

Clone the repository:

```bash
git clone https://github.com/yourusername/capita-selecta.git
cd capita-selecta
```

Create and activate a virtual environment:

```bash
python -m venv venv
source venv/bin/activate   # on Mac/Linux
venv\Scripts\activate      # on Windows
```

Install dependencies:

```bash
pip install -r requirements.txt
```

## ğŸš€ Usage

Open the Jupyter notebook:

```bash
jupyter notebook Capita_Selecta_Federico_Benetti.ipynb
```

Run the cells step by step to:
1. Scrape Italian text data  
2. Clean and preprocess text  
3. Apply LDA topic modeling  
4. Visualize insights  

## ğŸ“Š Example Output

- Distribution of topics across documents  
- Interactive LDA visualization via **pyLDAvis**  
- Word clouds & keyword frequency plots  

## ğŸ“‚ Repository Structure

```
â”œâ”€â”€ Capita_Selecta_Federico_Benetti.ipynb   # Main analysis notebook
â”œâ”€â”€ requirements.txt                        # Python dependencies
â”œâ”€â”€ README.md                               # Project description
â”œâ”€â”€ .gitignore                              # Ignore unnecessary files
â”œâ”€â”€ data/                                   # (Optional) Store scraped/cleaned data
â””â”€â”€ results/                                # (Optional) Save plots and models
```

